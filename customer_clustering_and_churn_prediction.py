# -*- coding: utf-8 -*-
"""Customer_Clustering_and_Churn_Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dlLtx2ikZsf059zB3c2w4-wHqOKzx6GO

# Loading and Preprocessing
"""

iimport numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import missingno as msno
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.manifold import TSNE
from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import KMeans
import faiss
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

!pip install openpyxl

!pip install missingpy

df=pd.read_excel('/content/E Commerce Dataset.xlsx')
df.head()

df.info()

df.isnull().sum()

# Correlation heatmap for numeric columns
numeric_df = df.select_dtypes(include=['int64', 'float64'])
plt.figure(figsize=(7, 7))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')
plt.show()

import missingno as msno
import matplotlib.pyplot as plt

msno.matrix(df)  # Visualize missing data
plt.show()

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imputer = IterativeImputer()
df[['Tenure', 'WarehouseToHome', 'HourSpendOnApp','OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount','DaySinceLastOrder']] = imputer.fit_transform(df[['Tenure', 'WarehouseToHome', 'HourSpendOnApp','OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount','DaySinceLastOrder']])

# Apply imputation

# Display missing values count after imputation
print("Missing values after imputation:\n", df.isnull().sum())

# Histogram for numerical features
df.hist(figsize=(10, 8), bins=20)
plt.show()

"""churn,"""

# Identify numeric columns except 'Churn'
numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
numeric_cols.remove('Churn')

# Standardize the data
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

df.hist(figsize=(10, 8), bins=20)
plt.show()

object_cols = df.select_dtypes(include=['object']).columns
df_enc = pd.get_dummies(df, columns=object_cols)

print(df_enc.info())

df1=df_enc.copy()

df2=df_enc.copy()

"""# Clustering Technique 1"""

pca = PCA(n_components=0.95)  # Retain 95% of variance
data_reduced = pca.fit_transform(df1)

print(data_reduced.shape[1])  # Number of principal components retained

from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score
import matplotlib.pyplot as plt

# Define a function to find the optimal k
def find_optimal_k(data):
    silhouette_scores = []
    calinski_scores = []
    k_range = range(2, 21)  # Try clustering for k=2 to k=10 clusters

    for k in k_range:
        # Fit the Gaussian Mixture Model
        gmm = GaussianMixture(n_components=k, random_state=42)
        gmm.fit(data)

        # Predict cluster labels
        labels = gmm.predict(data)

        # Calculate Silhouette Score and Calinski-Harabasz Index
        silhouette = silhouette_score(data, labels)
        calinski = calinski_harabasz_score(data, labels)

        silhouette_scores.append(silhouette)
        calinski_scores.append(calinski)

    # Plot the results
    plt.figure(figsize=(10, 5))

    # Silhouette Scores plot
    plt.subplot(1, 2, 1)
    plt.plot(k_range, silhouette_scores, marker='o', color='b', label='Silhouette Score')
    plt.title('Silhouette Score vs. k')
    plt.xlabel('Number of Clusters (k)')
    plt.ylabel('Silhouette Score')

    # Calinski-Harabasz Scores plot
    plt.subplot(1, 2, 2)
    plt.plot(k_range, calinski_scores, marker='o', color='g', label='Calinski-Harabasz Score')
    plt.title('Calinski-Harabasz Score vs. k')
    plt.xlabel('Number of Clusters (k)')
    plt.ylabel('Calinski-Harabasz Score')

    plt.tight_layout()
    plt.show()

    # Return the optimal k based on the highest score (can use either metric)
    optimal_k_silhouette = k_range[silhouette_scores.index(max(silhouette_scores))]
    optimal_k_calinski = k_range[calinski_scores.index(max(calinski_scores))]

    print(f"Optimal k based on Silhouette Score: {optimal_k_silhouette}")
    print(f"Optimal k based on Calinski-Harabasz Score: {optimal_k_calinski}")

# Find optimal k using the dataset
find_optimal_k(data_reduced)

# Fit the final GMM model with the optimal k
optimal_k = 2

gmm = GaussianMixture(n_components=optimal_k, random_state=42)
gmm.fit(data_reduced)
labels = gmm.predict(data_reduced)

# Print the first 10 cluster assignments
print(labels[:10])

from sklearn.manifold import TSNE

# Reduce the data using t-SNE
tsne = TSNE(n_components=2, random_state=42)
data_tsne = tsne.fit_transform(data_reduced)

# Plot the t-SNE reduced data
plt.scatter(data_tsne[:, 0], data_tsne[:, 1], c=labels, cmap='viridis')
plt.title(f'T-SNE Clustering (k={optimal_k})')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.show()


# Optionally, you can visualize the clustering (for 2D data or with dimensionality reduction like PCA)
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Reduce the data to 2D for visualization
pca = PCA(n_components=2)
data_2d = pca.fit_transform(data_reduced)

plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='viridis')
plt.title(f'GMM Clustering (k={optimal_k})')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

from mpl_toolkits.mplot3d import Axes3D


tsne = TSNE(n_components=3, random_state=42)
data_tsne = tsne.fit_transform(data_reduced)

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(data_tsne[:, 0], data_tsne[:, 1], data_tsne[:, 2], c=labels, cmap='viridis')
ax.set_title(f'3D t-SNE Clustering (k={optimal_k})')
ax.set_xlabel('t-SNE Component 1')
ax.set_ylabel('t-SNE Component 2')
ax.set_zlabel('t-SNE Component 3')

# Create legend
legend1 = ax.legend(*scatter.legend_elements(),
                    loc="lower left", title="Clusters")
ax.add_artist(legend1)
plt.show()

"""# Clustering Technique 2"""

pca = PCA(n_components=0.90)  # Retain 95% of variance
data_reduced1 = pca.fit_transform(df2)

data_np = np.array(data_reduced1).astype('float32')

!pip install faiss-cpu

from sklearn.cluster import KMeans

# Define a function to find the optimal number of clusters (k)
def find_optimal_k(data_np, max_k=20):
    inertia = []
    for k in range(1, max_k + 1):
        # KMeans clustering
        kmeans = KMeans(n_clusters=k, random_state=42)
        kmeans.fit(data_np)
        inertia.append(kmeans.inertia_)  # Store the inertia (within-cluster sum of squares)

    # Plot inertia to find the "elbow"
    plt.plot(range(1, max_k + 1), inertia)
    plt.title('Elbow Method For Optimal k')
    plt.xlabel('Number of clusters (k)')
    plt.ylabel('Inertia')
    plt.show()

# Find optimal k
find_optimal_k(data_np)

import faiss
# Create the FAISS index for ANN search
index = faiss.IndexFlatL2(data_np.shape[1])  # L2 distance metric
index.add(data_np)  # Add data to the index

# Perform KMeans clustering using FAISS
def faiss_kmeans(data_np, k):
    # Initialize k-means
    kmeans = faiss.Kmeans(d=data_np.shape[1], k=k, niter=20, verbose=True)
    kmeans.train(data_np)
    return kmeans

# Example for k = 4 (use the optimal k from the previous step)
kmeans_model = faiss_kmeans(data_np, k=4)

# Cluster assignments
labels_faiss = kmeans_model.index.search(data_np, 1)[1]  # Get the cluster assignments

# Print the first 10 cluster assignments
print(labels_faiss[:20])

"""# Churn Prediction"""

df3=df_enc.copy()

df3['Cluster_gmm']=labels

X = df3.drop(columns=['Churn'])  # Exclude target variable 'Churn'
y = df3['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
model_rf = RandomForestClassifier(random_state=42)
model_rf.fit(X_train, y_train)

model_lr = LogisticRegression(random_state=42)
model_lr.fit(X_train, y_train)

y_pred_logreg = model_lr.predict(X_test)

# Evaluate Model
logreg_accuracy = accuracy_score(y_test, y_pred_logreg)
print(f'Logistic Regression Model Accuracy: {logreg_accuracy:.4f}')
print(classification_report(y_test, y_pred_logreg))

y_pred_rf = model_rf.predict(X_test)

# Evaluate Model
rf_accuracy = accuracy_score(y_test, y_pred_rf)
print(f'Random Forest Model Accuracy: {rf_accuracy:.4f}')
print(classification_report(y_test, y_pred_rf))

import pandas as pd

# Get probability predictions for both models
y_probs_logreg = model_lr.predict_proba(X_test)[:, 1]  # Logistic Regression probabilities
y_probs_rf = model_rf.predict_proba(X_test)[:, 1]  # XGBoost probabilities



# Create a DataFrame with actual churn and predictions
predictions_df = pd.DataFrame({
    'Actual_Churn': y_test.values,
    'LogReg_Pred': y_pred_logreg,
    'XGBoost_Pred': y_pred_rf,
    'LogReg_Prob': y_probs_logreg,  # Optional: Include probability scores
    'XGBoost_Prob': y_probs_rf
})

# Save to CSV
predictions_df.to_csv('churn_predictions_gmm.csv', index=False)

print("Predictions saved as churn_predictions.csv")